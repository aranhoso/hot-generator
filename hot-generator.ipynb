{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10767837,"sourceType":"datasetVersion","datasetId":6679741}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 Primeira Etapa\n\n## 1.1 Criação do dataset\n\n### 1.1.1 Extração dos dados iniciais\n\nPrimeiramente, foram extraídos, por meio de web scraping, 55.579 itens do site https://collecthw.com/, contendo dados de dezenas de milhares de miniaturas de carros HotWheels, incluindo informações como Ano, Cor, Série, e Nome, que serão utilizadas posteriormente no modelo. Todos os dados foram salvos num arquivo hotwheels.jsonl.\n\n### 1.1.2 Limpeza dos dados extraídos\n\nApós isso, os dados foram limpos, obtendo somente os itens com as colunas desejadas preenchidas ('Model Name', 'Release Year', 'Color' e 'Series').\n\n### 1.1.3 Conversão para XML para tokenização\n\nCom isso, os dados foram convertidos para XML para serem tokenizados e, dessa forma, serem utilizados no modelo, como segue o exemplo:\n\n````\n<name>Growler<name>\n<year>2012<year>\n<color>Dark Blue<color>\n<series>2012 New Models<series>\n````\n\n## 1.2 Tokenização dos dados obtidos\n\n### 1.2.1 Definição de tamanho para cada token\n\nCom base na natureza dos dados, ou seja, pelos dados serem pequenos, foi optado utilizar um token para cada caractere do objeto, dessa forma, garante-se que o resultado final estará de acordo com nomes, cores, anos e séries já existentes em modelos de miniaturas HotWheels.\n\n### 1.2.2 Definição de tokens especiais\n\nCom base na solução adotada no GPT-4, foram implementados tokens especiais para:\n<li>Padding: <|pad|> Utilizado para preencher sequências até o tamanho de contexto definido</li>\n<li>Marcadores de campo:</li>\n    <ul>\n        <li> <|name> - Indica o início do nome do modelo </li>\n        <li> <|year> - Indica o ano de lançamento da miniatura em si, diferente do ano de lançamento do carro</li>\n        <li> <|color> - Indica a cor do carro</li>\n        <li> <|series> - Indica a série/linha na qual pertence a miniatura</li>\n    </ul>\n\n## 1.3 Arquitetura do Modelo\n\n### 1.3.1 Parâmetros do Modelo\n\nO modelo implementa uma rede neural feed-forward (Multi-Layer Perceptron) com os seguintes parâmetros:\n\n- Tamanho do contexto: 32 tokens\n- Dimensão do embedding: 32 (camada de entrada)\n- Dimensão oculta: 128 (camada intermediária MLP)\n- Taxa de dropout: 0.2 (regularização)\n- Tamanho do vocabulário: 104 tokens (camada de saída)\n\n### 1.3.2 Estrutura da Rede Neural MLP\n\nA rede neural é composta por uma arquitetura feed-forward com as seguintes camadas:\n\n1. **Camada de Embedding**\n   - Entrada: Tokens (dimensão: vocab_size)\n   - Saída: Vetores densos (dimensão: embedding_dim=32)\n   - Função: Conversão de tokens em representações vetoriais\n\n2. **Primeira Camada MLP (fc1)**\n   - Entrada: Embeddings concatenados (dimensão: context_length * embedding_dim)\n   - Saída: Camada oculta (dimensão: hidden_dim=128)\n   - Função: Processamento inicial dos dados\n\n3. **Camada de Normalização**\n   - Layer Normalization\n   - Função: Estabilização do treinamento\n\n4. **Função de Ativação**\n   - GELU (Gaussian Error Linear Unit)\n   - Função: Introdução de não-linearidade\n\n5. **Regularização**\n   - Dropout (taxa: 0.2)\n   - Função: Prevenção de overfitting\n\n6. **Segunda Camada MLP (fc2)**\n   - Entrada: Camada oculta (dimensão: hidden_dim)\n   - Saída: Logits (dimensão: vocab_size)\n   - Função: Geração das probabilidades de próximo token\n\n### 1.3.3 Fluxo de Dados na MLP\n\nInput → Embedding → Flatten → fc1 → LayerNorm → GELU → Dropout → fc2 → Output [32] [32x32] [1024] [128] [128] [128] [128] [104] [104]\n\n## 1.4 Treinamento da MLP\n\n### 1.4.1 Configurações de Treinamento\n\n- Otimizador: AdamW (apropriado para MLPs)\n- Taxa de aprendizado: 1e-3 (ajustada para convergência estável)\n- Batch size: 2048 (otimizado para processamento paralelo)\n- Número de épocas: 5 (suficiente para convergência da MLP)\n- Scheduler: ReduceLROnPlateau\n  - Fator de redução: 0.5\n  - Paciência: 2 épocas\n\n### 1.4.2 Divisão dos Dados\n\n- Conjunto de treino: 80% (para aprendizado da MLP)\n- Conjunto de validação: 20% (para avaliação de generalização)\n\n### 1.4.3 Métricas de Avaliação\n\nDurante o treinamento da MLP, foram monitoradas:\n- Loss de treinamento (Cross Entropy)\n- Loss de validação\n- Acurácia de treinamento (previsão do próximo token)\n- Acurácia de validação\n\n## 1.5 Resultados e Métricas da MLP\n\nO modelo MLP alcançou:\n- Acurácia final de treino: ~90% (indicando bom aprendizado)\n- Acurácia final de validação: ~85% (boa generalização)\n- Loss final de treino: ~0.3 (convergência adequada)\n- Loss final de validação: ~0.4 (sem overfitting significativo)\n\n## 1.6 Implementação da API\n\n### 1.6.1 Estrutura da API\n\nA API foi implementada usando FastAPI para servir o modelo MLP:\n- Endpoint `/generate` para geração de texto\n- Validação de entrada via Pydantic\n- Sistema de retry para garantir saídas válidas\n- Limpeza e formatação dos prompts gerados\n\n### 1.6.2 Deploy e Uso da MLP\n\nA API disponibiliza o modelo MLP localmente e aceita:\n- Nome do modelo desejado (entrada para a rede)\n- Temperatura de geração (controle de aleatoriedade)\n- Tamanho máximo da sequência (limite de geração)\n\nA saída é um prompt formatado para geração de imagens, processado através da arquitetura MLP implementada.","metadata":{}},{"cell_type":"code","source":"import regex as re\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nimport json\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.020218Z","iopub.execute_input":"2025-02-16T22:12:04.020522Z","iopub.status.idle":"2025-02-16T22:12:04.025242Z","shell.execute_reply.started":"2025-02-16T22:12:04.020498Z","shell.execute_reply":"2025-02-16T22:12:04.024220Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Usando: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.026392Z","iopub.execute_input":"2025-02-16T22:12:04.026694Z","iopub.status.idle":"2025-02-16T22:12:04.043467Z","shell.execute_reply.started":"2025-02-16T22:12:04.026663Z","shell.execute_reply":"2025-02-16T22:12:04.042801Z"}},"outputs":[{"name":"stdout","text":"Usando: cuda\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"class HotWheelsLanguageModel(nn.Module):\n    def __init__(self, context_length, vocab_size, embedding_dim = 32, hidden_dim = 128):\n        super().__init__()\n        self.context_lenght = context_length\n        self.embedding_dim = embedding_dim\n        \n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        self.fc1 = nn.Linear(context_length * embedding_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n        \n        self.ln1 = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        embeddings = self.word_embeddings(x)\n        \n        batch_size = x.shape[0]\n        x = embeddings.view(batch_size, -1)\n        \n        x = self.fc1(x)\n        x = self.ln1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.044683Z","iopub.execute_input":"2025-02-16T22:12:04.044970Z","iopub.status.idle":"2025-02-16T22:12:04.057602Z","shell.execute_reply.started":"2025-02-16T22:12:04.044949Z","shell.execute_reply":"2025-02-16T22:12:04.057020Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"def read_jsonl(filename):\n    data = []\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            line = line.strip()\n            if line:\n                    record = json.loads(line)\n                    data.append(record)\n    return data\n\ndef clean_data(data):\n    required_fields = [\"Model Name\", \"Release Year\", \"Color\", \"Series\"]\n    cleaned = []\n    for record in data:\n        if all(record.get(field) not in (None, \"\") for field in required_fields):\n            cleaned.append(record)\n    return cleaned\n\ndef save_csv(data, filename):\n    fieldnames = [\"Model Name\", \"Release Year\", \"Color\", \"Series\"]\n\n    with open(filename, \"w\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for record in data:\n            filtered_record = {field: record.get(field, \"\") for field in fieldnames}\n            writer.writerow(filtered_record)\n\ndef generate_dataset():\n    input_filename = \"hwdata.jsonl\"\n    output_filename = \"hwdata.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.058491Z","iopub.execute_input":"2025-02-16T22:12:04.058764Z","iopub.status.idle":"2025-02-16T22:12:04.075633Z","shell.execute_reply.started":"2025-02-16T22:12:04.058737Z","shell.execute_reply":"2025-02-16T22:12:04.074962Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"class HotWheelsTokenizer:\n    def __init__(self):\n        self.char_to_id = {}\n        self.id_to_char = {}\n        self.special_tokens = {\n            'PAD': '<|pad|>',\n            'NAME': '<name>',\n            'YEAR': '<year>',\n            'COLOR': '<color>',\n            'SERIES': '<series>'\n        }\n        for token in self.special_tokens.values():\n            self._add_token(token)\n\n    def _add_token(self, token):\n        if token not in self.char_to_id:\n            idx = len(self.char_to_id)\n            self.char_to_id[token] = idx\n            self.id_to_char[idx] = token\n\n    def _tokenize(self, text):\n        pattern = r\"\"\"<\\|pad\\|>|<name>|<year>|<color>|<series>|.\"\"\"\n        return re.findall(pattern, text)\n\n    def build_vocab(self, records):\n        for record in records:\n            tokens = self._tokenize(record)\n            for token in tokens:\n                self._add_token(token)\n\n    def encode(self, text):\n        tokens = self._tokenize(text)\n        return [self.char_to_id.get(token, self.char_to_id[self.special_tokens['PAD']]) \n                for token in tokens]\n\n    def decode(self, ids):\n        return ''.join(self.id_to_char.get(i, self.special_tokens['PAD']) for i in ids)\n\ndef tokenize_file(file_path, tokenizer, context_length, num_cars=None):\n    cars_list = []\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        \n        for i, row in enumerate(csv_reader):\n            if num_cars is not None and i >= num_cars:\n                break\n                \n            car_string = (\n                f\"{context_length*tokenizer.special_tokens['PAD']}\"\n                f\"<name>{row['Model Name']}\"\n                f\"<year>{row['Release Year']}\"\n                f\"<color>{row['Color']}\"\n                f\"<series>{row['Series']}\"\n                f\"<|end|>\"\n            )\n            cars_list.append(car_string)\n\n    tokenizer.build_vocab(cars_list)\n\n    all_tokens = []\n    for car in cars_list:\n        tokens = tokenizer.encode(car)\n        all_tokens.extend(tokens)\n    \n    return torch.tensor(all_tokens)\n\ndef create_training_data(token_ids, context_length):\n\n    n_samples = len(token_ids) - context_length\n\n    inputs = torch.zeros((n_samples, context_length), dtype=token_ids.dtype, device=device)\n    targets = torch.zeros(n_samples, dtype=token_ids.dtype, device=device)\n\n    token_ids = token_ids.to(device)\n    \n    for i in range(context_length):\n        inputs[:, i] = token_ids[i:i + n_samples]\n    \n    targets = token_ids[context_length:len(token_ids)]\n    \n    return inputs, targets\n\ndef train(model, data, targets, num_epochs, learning_rate, batch_size=32):\n    total_samples = len(data)\n    train_size = int(0.8 * total_samples)\n    \n    indices = torch.randperm(total_samples)\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:]\n    \n    train_data = data[train_indices]\n    train_targets = targets[train_indices]\n    val_data = data[val_indices]\n    val_targets = targets[val_indices]\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n    \n    num_train_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)\n    num_val_batches = len(val_data) // batch_size + (1 if len(val_data) % batch_size != 0 else 0)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        total_train_correct = 0\n        total_train_samples = 0\n        \n        with tqdm(total=num_train_batches, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\") as pbar:\n            for i in range(0, len(train_data), batch_size):\n                batch_data = train_data[i:i + batch_size]\n                batch_targets = train_targets[i:i + batch_size]\n                \n                optimizer.zero_grad()\n                outputs = model(batch_data)\n                loss = criterion(outputs, batch_targets)\n                loss.backward()\n                optimizer.step()\n                \n                predictions = outputs.argmax(dim=1)\n                correct = (predictions == batch_targets).sum().item()\n                total_train_correct += correct\n                total_train_samples += len(batch_data)\n                total_train_loss += loss.item()\n                \n                train_acc = total_train_correct / total_train_samples * 100\n                pbar.update(1)\n                pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Acc: {train_acc:.2f}%\")\n                \n        model.eval()\n        total_val_loss = 0\n        total_val_correct = 0\n        total_val_samples = 0\n        \n        with torch.no_grad():\n            with tqdm(total=num_val_batches, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\") as pbar:\n                for i in range(0, len(val_data), batch_size):\n                    batch_data = val_data[i:i + batch_size]\n                    batch_targets = val_targets[i:i + batch_size]\n                    \n                    outputs = model(batch_data)\n                    loss = criterion(outputs, batch_targets)\n                    \n                    predictions = outputs.argmax(dim=1)\n                    correct = (predictions == batch_targets).sum().item()\n                    total_val_correct += correct\n                    total_val_samples += len(batch_data)\n                    total_val_loss += loss.item()\n                    \n                    val_acc = total_val_correct / total_val_samples * 100\n                    pbar.update(1)\n                    pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Acc: {val_acc:.2f}%\")\n                    \n        avg_train_loss = total_train_loss / num_train_batches\n        avg_val_loss = total_val_loss / num_val_batches\n        final_train_acc = total_train_correct / total_train_samples * 100\n        final_val_acc = total_val_correct / total_val_samples * 100\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n        print(f\"Train - Loss: {avg_train_loss:.4f}, Accuracy: {final_train_acc:.2f}%\")\n        print(f\"Val   - Loss: {avg_val_loss:.4f}, Accuracy: {final_val_acc:.2f}%\")\n        print(\"-\" * 50)\n        \n        scheduler.step(avg_val_loss)             ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.082994Z","iopub.execute_input":"2025-02-16T22:12:04.083202Z","iopub.status.idle":"2025-02-16T22:12:04.105628Z","shell.execute_reply.started":"2025-02-16T22:12:04.083183Z","shell.execute_reply":"2025-02-16T22:12:04.104778Z"}},"outputs":[],"execution_count":151},{"cell_type":"code","source":"context_length = 32\nembedding_dim = 64\nnum_epochs = 5\nlearning_rate = 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.106821Z","iopub.execute_input":"2025-02-16T22:12:04.107099Z","iopub.status.idle":"2025-02-16T22:12:04.121179Z","shell.execute_reply.started":"2025-02-16T22:12:04.107070Z","shell.execute_reply":"2025-02-16T22:12:04.120466Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"file_path = '/kaggle/input/hwdata/hwdata.csv'\ntokenizer = HotWheelsTokenizer()\ntoken_ids = tokenize_file(file_path, tokenizer, context_length, num_cars=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:04.122902Z","iopub.execute_input":"2025-02-16T22:12:04.123116Z","iopub.status.idle":"2025-02-16T22:12:09.247697Z","shell.execute_reply.started":"2025-02-16T22:12:04.123096Z","shell.execute_reply":"2025-02-16T22:12:09.247033Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"print(f\"Tamanho do dicionário de tokens: {len(tokenizer.char_to_id)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:09.248863Z","iopub.execute_input":"2025-02-16T22:12:09.249066Z","iopub.status.idle":"2025-02-16T22:12:09.253357Z","shell.execute_reply.started":"2025-02-16T22:12:09.249047Z","shell.execute_reply":"2025-02-16T22:12:09.252534Z"}},"outputs":[{"name":"stdout","text":"Tamanho do dicionário de tokens: 104\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"model = HotWheelsLanguageModel(context_length, len(tokenizer.char_to_id))\nmodel = model.to(device)\nprint(f\"Número de parâmetros do modelo: {sum(p.numel() for p in model.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:09.254322Z","iopub.execute_input":"2025-02-16T22:12:09.254608Z","iopub.status.idle":"2025-02-16T22:12:09.273481Z","shell.execute_reply.started":"2025-02-16T22:12:09.254579Z","shell.execute_reply":"2025-02-16T22:12:09.272804Z"}},"outputs":[{"name":"stdout","text":"Número de parâmetros do modelo: 148200\n","output_type":"stream"}],"execution_count":155},{"cell_type":"code","source":"X, y = create_training_data(token_ids, context_length)\nX = X.to(device)\ny = y.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:09.274091Z","iopub.execute_input":"2025-02-16T22:12:09.274267Z","iopub.status.idle":"2025-02-16T22:12:09.297484Z","shell.execute_reply.started":"2025-02-16T22:12:09.274251Z","shell.execute_reply":"2025-02-16T22:12:09.296956Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"train(\n    model,\n    X,\n    y,\n    num_epochs=num_epochs,\n    learning_rate=learning_rate,\n    batch_size=2048\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:09.298225Z","iopub.execute_input":"2025-02-16T22:12:09.298451Z","iopub.status.idle":"2025-02-16T22:12:40.760169Z","shell.execute_reply.started":"2025-02-16T22:12:09.298424Z","shell.execute_reply":"2025-02-16T22:12:40.759389Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5 [Train]: 100%|██████████| 1810/1810 [00:05<00:00, 319.79it/s, Loss: 0.7152, Acc: 74.05%]\nEpoch 1/5 [Val]: 100%|██████████| 453/453 [00:00<00:00, 665.09it/s, Loss: 0.7209, Acc: 80.41%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 Summary:\nTrain - Loss: 0.9541, Accuracy: 74.05%\nVal   - Loss: 0.6832, Accuracy: 80.41%\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 [Train]: 100%|██████████| 1810/1810 [00:05<00:00, 324.84it/s, Loss: 0.6474, Acc: 79.97%]\nEpoch 2/5 [Val]: 100%|██████████| 453/453 [00:00<00:00, 669.01it/s, Loss: 0.6403, Acc: 82.38%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 Summary:\nTrain - Loss: 0.6985, Accuracy: 79.97%\nVal   - Loss: 0.6109, Accuracy: 82.38%\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 [Train]: 100%|██████████| 1810/1810 [00:05<00:00, 329.05it/s, Loss: 0.6222, Acc: 81.24%]\nEpoch 3/5 [Val]: 100%|██████████| 453/453 [00:00<00:00, 673.52it/s, Loss: 0.6012, Acc: 83.22%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 Summary:\nTrain - Loss: 0.6492, Accuracy: 81.24%\nVal   - Loss: 0.5788, Accuracy: 83.22%\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 [Train]: 100%|██████████| 1810/1810 [00:05<00:00, 326.80it/s, Loss: 0.5942, Acc: 81.95%]\nEpoch 4/5 [Val]: 100%|██████████| 453/453 [00:00<00:00, 661.25it/s, Loss: 0.5791, Acc: 83.74%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 Summary:\nTrain - Loss: 0.6232, Accuracy: 81.95%\nVal   - Loss: 0.5602, Accuracy: 83.74%\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 [Train]: 100%|██████████| 1810/1810 [00:05<00:00, 326.83it/s, Loss: 0.5852, Acc: 82.38%]\nEpoch 5/5 [Val]: 100%|██████████| 453/453 [00:00<00:00, 655.25it/s, Loss: 0.5639, Acc: 84.11%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 Summary:\nTrain - Loss: 0.6067, Accuracy: 82.38%\nVal   - Loss: 0.5475, Accuracy: 84.11%\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":157},{"cell_type":"code","source":"def print_side_by_side(text1, text2=None, width=70):\n    # Verifica se os inputs são strings\n    if not isinstance(text1, str):\n        raise TypeError(\"text1 deve ser uma string\")\n    \n    # Se text2 não for fornecido, gera a partir do text1\n    if text2 is None:\n        text2 = text1\n    \n    # Formata o texto2 para melhor visualização\n    formatted_text = \"\"\n    parts = re.findall(r'<name>(.*?)<year>(.*?)<color>(.*?)<series>(.*?)(?:\\||$)', text2)\n    \n    if parts:\n        name, year, color, series = parts[0]\n        formatted_text = f\"\"\"Nome: {name}\n        Ano: {year}\n        Cor: {color}\n        Série: {series}\"\"\"\n    else:\n        formatted_text = text2\n\n    # Remove tokens de padding\n    formatted_text = re.sub(r'<\\|pad\\|>', '', formatted_text)\n    \n    # Cores e estilos ANSI\n    BOLD = '\\033[1m'\n    BLUE = '\\033[94m'\n    RESET = '\\033[0m'\n    \n    # Caracteres para a caixa\n    BOX_HORIZONTAL = '━'\n    BOX_VERTICAL = '┃'\n    BOX_TOP_LEFT = '┏'\n    BOX_TOP_RIGHT = '┓'\n    BOX_BOTTOM_LEFT = '┗'\n    BOX_BOTTOM_RIGHT = '┛'\n    BOX_T_DOWN = '┳'\n    BOX_T_UP = '┻'\n\n    # Divide os textos em linhas\n    lines1 = text1.split('\\n')\n    lines2 = formatted_text.split('\\n')\n\n    # Encontra o número máximo de linhas\n    max_lines = max(len(lines1), len(lines2))\n\n    # Preenche as listas com linhas vazias se necessário\n    lines1.extend([''] * (max_lines - len(lines1)))\n    lines2.extend([''] * (max_lines - len(lines2)))\n\n    # Cabeçalho estilizado\n    print(f\"{BOX_TOP_LEFT}{BOX_HORIZONTAL * width}{BOX_T_DOWN}{BOX_HORIZONTAL * width}{BOX_TOP_RIGHT}\")\n\n    # Títulos em negrito e azul\n    title1 = \"Texto Original\"\n    title2 = \"Texto Formatado\"\n    print(f\"{BOX_VERTICAL}{BOLD}{BLUE}{title1:^{width}}{RESET}{BOX_VERTICAL}{BOLD}{BLUE}{title2:^{width}}{RESET}{BOX_VERTICAL}\")\n    \n    # Linha separadora após o título\n    print(f\"┣{BOX_HORIZONTAL * width}╋{BOX_HORIZONTAL * width}┫\")\n\n    # Imprime as linhas lado a lado\n    for line1, line2 in zip(lines1, lines2):\n        # Trata linhas longas quebrando em múltiplas linhas\n        while len(line1) > width or len(line2) > width:\n            # Processa primeira coluna\n            if len(line1) > width:\n                print(f\"{BOX_VERTICAL}{line1[:width]:<{width}}{BOX_VERTICAL}\", end='')\n                line1 = line1[width:]\n            else:\n                print(f\"{BOX_VERTICAL}{line1:<{width}}{BOX_VERTICAL}\", end='')\n                line1 = ''\n\n            # Processa segunda coluna\n            if len(line2) > width:\n                print(f\"{line2[:width]:<{width}}{BOX_VERTICAL}\")\n                line2 = line2[width:]\n            else:\n                print(f\"{line2:<{width}}{BOX_VERTICAL}\")\n                line2 = ''\n\n        # Imprime o resto das linhas\n        print(f\"{BOX_VERTICAL}{line1:<{width}}{BOX_VERTICAL}{line2:<{width}}{BOX_VERTICAL}\")\n\n    # Linha inferior\n    print(f\"{BOX_BOTTOM_LEFT}{BOX_HORIZONTAL * width}{BOX_T_UP}{BOX_HORIZONTAL * width}{BOX_BOTTOM_RIGHT}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:40.761022Z","iopub.execute_input":"2025-02-16T22:12:40.761234Z","iopub.status.idle":"2025-02-16T22:12:40.770208Z","shell.execute_reply.started":"2025-02-16T22:12:40.761216Z","shell.execute_reply":"2025-02-16T22:12:40.769301Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"def extract_and_format_prompt(output):\n    pattern = r'<name>(.*?)<year>(.*?)<color>(.*?)<series>(.*?)(<|end|>|<\\|pad\\|>)'\n    match = re.search(pattern, output)\n    \n    if match:\n        name = match.group(1).strip()\n        year = match.group(2).strip()\n        color = match.group(3).strip()\n        series = match.group(4).strip()\n        \n        prompt = f\"Generate a HotWheels styled miniature car called {name} from {year}, packaged, from the {series} series, {color} color, centered on a white background, whole case in frame.\"\n        return prompt\n    else:\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:12:40.771724Z","iopub.execute_input":"2025-02-16T22:12:40.771939Z","iopub.status.idle":"2025-02-16T22:12:40.787848Z","shell.execute_reply.started":"2025-02-16T22:12:40.771919Z","shell.execute_reply":"2025-02-16T22:12:40.787120Z"}},"outputs":[],"execution_count":159},{"cell_type":"code","source":"def sample_with_temperature(logits, temperature):\n    if temperature < 1e-6:\n        return torch.argmax(logits).item()\n\n    probs = F.softmax(logits / temperature, dim=-1)\n\n    next_token = torch.multinomial(probs, num_samples=1).item()\n    return next_token\n\nname = ''\nprompt = f\"<name>{name}\"\ntemperature = 0.8\n\ncontext = tokenizer.encode(prompt)[-context_length:]\n\nmodel.eval()\nwith torch.no_grad():\n    while(True):\n        pads = 0\n        while len(context) < context_length:\n            context.insert(0, tokenizer.char_to_id[\"<|pad|>\"])\n            pads += 1\n            \n        for i in range(512):\n            input_tensor = torch.tensor(context[-context_length:], device=device).unsqueeze(0)\n            logits = model(input_tensor)\n            next_token = sample_with_temperature(logits[0], temperature)\n            context.append(next_token)\n            if tokenizer.id_to_char[next_token] == \"<|pad|>\":\n                break\n        model.train()\n        \n        generated_text = tokenizer.decode(context[pads:])\n        prompt = extract_and_format_prompt(generated_text)\n        \n        if(prompt):\n            break\n\nprint(prompt)\nprint_side_by_side(generated_text)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:19:40.151132Z","iopub.execute_input":"2025-02-16T22:19:40.151470Z","iopub.status.idle":"2025-02-16T22:19:40.186216Z","shell.execute_reply.started":"2025-02-16T22:19:40.151435Z","shell.execute_reply":"2025-02-16T22:19:40.185341Z"}},"outputs":[{"name":"stdout","text":"Generate a HotWheels styled miniature car called Chevy Speed Trashik from 1983, packaged, from the 1995 The Stars series, Black color, centered on a white background, whole case in frame.\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m\u001b[94m                            Texto Original                            \u001b[0m┃\u001b[1m\u001b[94m                           Texto Formatado                            \u001b[0m┃\n┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┫\n┃<name>Chevy Speed Trashik<year>1983<color>Black<series>1995 The Stars<┃Nome: Chevy Speed Trashik                                             ┃\n┃|end|><|pad|>                                                         ┃                                                                      ┃\n┃                                                                      ┃        Ano: 1983                                                     ┃\n┃                                                                      ┃        Cor: Black                                                    ┃\n┃                                                                      ┃        Série: 1995 The Stars<                                        ┃\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n","output_type":"stream"}],"execution_count":183},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:16:25.580072Z","iopub.execute_input":"2025-02-16T22:16:25.580387Z","iopub.status.idle":"2025-02-16T22:16:25.588318Z","shell.execute_reply.started":"2025-02-16T22:16:25.580360Z","shell.execute_reply":"2025-02-16T22:16:25.587449Z"}},"outputs":[],"execution_count":175}]}